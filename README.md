 ### Desafio_Final_Big_Data
 
<h1 align="center"> Pipeline de Dados </h1>
<h4 align="center">Projeto final para o programa Jovens Profissionais BI/BA realizado pela Minsait Indra, com intuito de apresentar as habilidades adquiridas durante a jornada de aprendizado no programa.</h4>
<h4 tabindex="-1" dir="auto">üìå Principais tecnologias utilizadas</h4>

![MySQL](https://img.shields.io/badge/mysql-%2300f.svg?style=for-the-badge&logo=mysql&logoColor=white) ![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54) ![Shell Script](https://img.shields.io/badge/shell_script-%23121011.svg?style=for-the-badge&logo=gnu-bash&logoColor=white) ![Gitpod](https://img.shields.io/badge/gitpod-f06611.svg?style=for-the-badge&logo=gitpod&logoColor=white) ![Git](https://img.shields.io/badge/GIT-E44C30?style=for-the-badge&logo=git&logoColor=white) ![Jupyter Notebook](https://img.shields.io/badge/jupyter-%23FA0F00.svg?style=for-the-badge&logo=jupyter&logoColor=white) ![Linux](https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black) ![Apache](https://img.shields.io/badge/apache-%23D42029.svg?style=for-the-badge&logo=apache&logoColor=white)


<h4 tabindex="-1" dir="auto"></a> üìç Descri√ß√£o do projeto</h4>

Nesse projeto foi apresentado um desenvolvimento de um Pipeline de ingest√£o de dados utilizando Hive na arquitetura do Apache Hadoop juntamente com o processamento e tratamento dos dados com Spark. Projeto feito para ser usado pela √°rea de Business Intelligence utilizar para an√°lises no PowerBI e realizar a tomada de decis√µes


<h4 tabindex="-1" dir="auto">‚öíÔ∏è Constru√ß√£o do Projeto ‚öíÔ∏è</h4>
Para a movimenta√ß√£o e cria√ß√£o de pastas dentro do HDFS e tamb√©m da cria√ß√£o das tabelas externas no Hive foi utilizado ShellScript para a automa√ß√£o do processo.
<br></br>
Foram utilizadas tabelas de vendas, clientes, endere√ßo, regi√£o, divis√£o com esses dados foi necess√°rio realizar a desnormatiza√ß√£o das tabelas e transforma-las em um modelo dimensional de formato estrela.
Para isso, utilizando linguagem SQL dentro do Spark, foi realizado diversos joins para que fosse poss√≠vel chegar ao resultado esperado, al√©m da necessidade de tratar os dados de acordo com a necessidade do cliente, como por exemplo preencher campos string vazios com "N√£o informado", etc. Para nossa tabela stage, criamos as devidas DW keys, e em seguida realizando a cria√ß√£o e identifica√ß√£o da nossa Fato (Vendas) e Dimens√µes (Localidade, Tempo e Clientes).
<br></br>
Ap√≥s o tratamento de dados e a cria√ß√£o do nosso modelo estrela, com as tabelas tratadas, j√° na fase Gold do nosso DataLake, criamos nossa estrutura no PowerBI para que a √°rea de Business Intelligence consiga realizar consultas e an√°lises para tomada de decis√µes.
<br></br>

![image](https://user-images.githubusercontent.com/126920974/233228015-2469df4e-c3fa-4f63-b367-2d9ba2d08c2c.png)
![image](https://user-images.githubusercontent.com/126920974/236626739-c72be5be-c0e7-4bee-905c-d1d8dcc5008a.png)
<br></br>
<h4 tabindex="-1" dir="auto">üìö Principais conceitos aprendidos</h4>
<li> 5 Vs do Big Data e sua import√¢ncia para √°nalise de dados </li>
<li> Modelo estrela x Snowflake </li>
<li> Conceitos de Docker e Manipula√ß√£o dos cont√™ineres </li>
<li> Hadoop e sua arquitetura </li>
<li> Namenode x Datanode e sua rela√ß√£o com o HDFS </li>
<li> Comandos Linux </li>
<li> Conceitos de Datalake </li>
<li> ETL x ELT </li>
<li> Tabelas internas e externas no Hive </li>
<li> Processamento de dados no Spark utilizando Pyspark e SQL </li>
<li> Tabela stage e desnormatiza√ß√£o de dados </li>
<li> PowerBI para apresenta√ß√£o das informa√ß√µes </li>
<br></br>
üìå ESCOPO DO DESAFIO

Neste desafio ser√£o feitas as ingest√µes dos dados que est√£o na pasta /raw onde vamos ter alguns arquivos .csv de um banco relacional de vendas.

 - VENDAS.CSV
 - CLIENTES.CSV
 - ENDERECO.CSV
 - REGIAO.CSV
 - DIVISAO.CSV

Seu trabalho como engenheiro de dados/arquiteto de BI √© prover dados em uma pasta desafio_curso/gold em .csv para ser consumido por um relat√≥rio em PowerBI que dever√° ser constru√≠do dentro da pasta 'app' (j√° tem o template).

üìë ETAPAS

Etapa 1 - Enviar os arquivos para o HDFS
    - nesta etapa lembre de criar um shell script para fazer o trabalho repetitivo (n√£o √© obrigat√≥rio)

Etapa 2 - Criar o banco DEASFIO_CURSO e dentro tabelas no Hive usando o HQL e executando um script shell dentro do hive server na pasta scripts/pre_process.

    - DESAFIO_CURSO (nome do banco)
        - TBL_VENDAS
        - TBL_CLIENTES
        - TBL_ENDERECO
        - TBL_REGIAO
        - TBL_DIVISAO

Etapa 3 - Processar os dados no Spark Efetuando suas devidas transforma√ß√µes criando os arquivos com a modelagem de BI.
OBS. o desenvolvimento pode ser feito no jupyter porem no final o codigo deve estar no arquivo desafio_curso/scripts/process/process.py

Etapa 4 - Gravar as informa√ß√µes em tabelas dimensionais em formato cvs delimitado por ';'.

        - FT_VENDAS
        - DIM_CLIENTES
        - DIM_TEMPO
        - DIM_LOCALIDADE

Etapa 5 - Exportar os dados para a pasta desafio_curso/gold

Etapa 6 - Criar e editar o PowerBI com os dados que voc√™ trabalhou.

No PowerBI criar gr√°ficos de vendas.



REGRAS
Campos strings vazios dever√£o ser preenchidos com 'N√£o informado'.
Campos decimais ou inteiros nulos ou vazios, devers√£o ser preenchidos por 0.
Atentem-se a modelagem de dados da tabela FATO e Dimens√£o.
Na tabela FATO, pelo menos a m√©trica <b>valor de venda</b> √© um requisito obrigat√≥rio.
Nas dimens√µes dever√° conter valores √∫nicos, n√£o dever√° conter valores repetidos.

